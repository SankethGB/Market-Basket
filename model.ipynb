{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5. Model Building & Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SankethGB/Market-Basket/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tX82wydRhiD"
      },
      "source": [
        "Importing required Libraries.\n",
        "\n",
        "Kindly ensure that pytorch has been installed prioirly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZloeOkL_RPA2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "from mlxtend.frequent_patterns import apriori, association_rules"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2V-MBwRTelu"
      },
      "source": [
        "Import the cleaned dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-43I0ezaTbeb"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/outliers/Cleaned_Data.csv\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VL_gjI4EAet5",
        "outputId": "0b86a875-12d3-49b5-f672-7f62449dc14c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "R6nCINqhDq6L",
        "outputId": "4a33f46e-e657-48b9-f468-258d544a1e77"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Doc. Date</th>\n",
              "      <th>Material</th>\n",
              "      <th>Order qty</th>\n",
              "      <th>Ship-to nu</th>\n",
              "      <th>PCS delivered</th>\n",
              "      <th>HL delivered</th>\n",
              "      <th>delivery_days</th>\n",
              "      <th>delivery_flag</th>\n",
              "      <th>lead_time_creation_vs_rdd_flag</th>\n",
              "      <th>MACO/HL</th>\n",
              "      <th>Groupement</th>\n",
              "      <th>Postal Code</th>\n",
              "      <th>Street</th>\n",
              "      <th>Sous groupement</th>\n",
              "      <th>M2_Territory_ID</th>\n",
              "      <th>M1_Territory_ID</th>\n",
              "      <th>Dépt</th>\n",
              "      <th>Brand</th>\n",
              "      <th>Subrand</th>\n",
              "      <th>SEGMENTS : Pils / Spécialités / Superspécialités/Bouteille Young adult</th>\n",
              "      <th>Container Type</th>\n",
              "      <th>Container Size</th>\n",
              "      <th>Variétés</th>\n",
              "      <th>Segment LE</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>Degre Alc</th>\n",
              "      <th>TTC</th>\n",
              "      <th>Brut + TE</th>\n",
              "      <th>Net + TE - Hors majoration de rompu de palette</th>\n",
              "      <th>DA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-01-11</td>\n",
              "      <td>10946</td>\n",
              "      <td>5.0</td>\n",
              "      <td>29606863</td>\n",
              "      <td>350.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>150.689825</td>\n",
              "      <td>FRANCE BOISSONS</td>\n",
              "      <td>59160</td>\n",
              "      <td>RUE DU CHEMIN SAINT MARTIN</td>\n",
              "      <td>France BOISSONS</td>\n",
              "      <td>FR01_ON_1000</td>\n",
              "      <td>FR01_ON_1004</td>\n",
              "      <td>59.0</td>\n",
              "      <td>LEFFE</td>\n",
              "      <td>LEFFE BLONDE</td>\n",
              "      <td>Spécialités</td>\n",
              "      <td>PERFECT DRAFT</td>\n",
              "      <td>6,000 L</td>\n",
              "      <td>Blonde</td>\n",
              "      <td>PREMIUM</td>\n",
              "      <td>50.6455</td>\n",
              "      <td>2.9619</td>\n",
              "      <td>6.6</td>\n",
              "      <td>18.137448</td>\n",
              "      <td>15.582</td>\n",
              "      <td>15.114540</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-01-11</td>\n",
              "      <td>19898</td>\n",
              "      <td>2.0</td>\n",
              "      <td>29606863</td>\n",
              "      <td>140.0</td>\n",
              "      <td>8.4</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>141.207904</td>\n",
              "      <td>FRANCE BOISSONS</td>\n",
              "      <td>59160</td>\n",
              "      <td>RUE DU CHEMIN SAINT MARTIN</td>\n",
              "      <td>France BOISSONS</td>\n",
              "      <td>FR01_ON_1000</td>\n",
              "      <td>FR01_ON_1004</td>\n",
              "      <td>59.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>50.6455</td>\n",
              "      <td>2.9619</td>\n",
              "      <td>5.0</td>\n",
              "      <td>17.396467</td>\n",
              "      <td>14.688</td>\n",
              "      <td>14.497056</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2021-01-11</td>\n",
              "      <td>80176</td>\n",
              "      <td>4.0</td>\n",
              "      <td>29606863</td>\n",
              "      <td>280.0</td>\n",
              "      <td>16.8</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>92.872307</td>\n",
              "      <td>FRANCE BOISSONS</td>\n",
              "      <td>59160</td>\n",
              "      <td>RUE DU CHEMIN SAINT MARTIN</td>\n",
              "      <td>France BOISSONS</td>\n",
              "      <td>FR01_ON_1000</td>\n",
              "      <td>FR01_ON_1004</td>\n",
              "      <td>59.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>50.6455</td>\n",
              "      <td>2.9619</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.279859</td>\n",
              "      <td>10.368</td>\n",
              "      <td>10.233216</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2021-01-11</td>\n",
              "      <td>66989</td>\n",
              "      <td>1.0</td>\n",
              "      <td>29606863</td>\n",
              "      <td>70.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>195.334035</td>\n",
              "      <td>FRANCE BOISSONS</td>\n",
              "      <td>59160</td>\n",
              "      <td>RUE DU CHEMIN SAINT MARTIN</td>\n",
              "      <td>France BOISSONS</td>\n",
              "      <td>FR01_ON_1000</td>\n",
              "      <td>FR01_ON_1004</td>\n",
              "      <td>59.0</td>\n",
              "      <td>TRIPLE KARMELIET</td>\n",
              "      <td>TRIPLE KARMELIET</td>\n",
              "      <td>Craft</td>\n",
              "      <td>PERFECT DRAFT</td>\n",
              "      <td>6,000 L</td>\n",
              "      <td>-</td>\n",
              "      <td>SUPER PREMIUM</td>\n",
              "      <td>50.6455</td>\n",
              "      <td>2.9619</td>\n",
              "      <td>8.0</td>\n",
              "      <td>21.996000</td>\n",
              "      <td>18.330</td>\n",
              "      <td>18.330000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2021-01-08</td>\n",
              "      <td>11766</td>\n",
              "      <td>70.0</td>\n",
              "      <td>29378784</td>\n",
              "      <td>70.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>142.902378</td>\n",
              "      <td>INDEPENDANTS</td>\n",
              "      <td>62570</td>\n",
              "      <td>PLACE JEAN JAURES</td>\n",
              "      <td>ALLAN</td>\n",
              "      <td>FR01_ON_2000</td>\n",
              "      <td>FR01_ON_2006</td>\n",
              "      <td>62.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>50.6978</td>\n",
              "      <td>2.2423</td>\n",
              "      <td>5.2</td>\n",
              "      <td>17.359200</td>\n",
              "      <td>14.466</td>\n",
              "      <td>14.466000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...   DA\n",
              "0           0  ...  0.0\n",
              "1           1  ...  0.0\n",
              "2           2  ...  0.0\n",
              "3           3  ...  0.0\n",
              "4           4  ...  0.0\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2_4triNTUvb"
      },
      "source": [
        "List out all unique values of Groupement,material IDs and wholesaler IDs  in dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iHEYpMnT5U9"
      },
      "source": [
        "group1 =dataset.iloc[:, 12].unique()\n",
        "setter1 = set(group1)\n",
        "groups = (list(setter1))\n",
        "materials1 =dataset.iloc[:, 3].values\n",
        "setter = set(materials1)\n",
        "materials = (list(setter))\n",
        "buyers1 = dataset.iloc[:, 5].values\n",
        "settt = set(buyers1)\n",
        "buyers = (list(settt))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akb17mDvcQGm"
      },
      "source": [
        "***One with the apriori algorithm***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v4oZs3HXzsM"
      },
      "source": [
        "\n",
        "\n",
        "*   Reducing the entire dataset selecting the columns which are useful for the process input\n",
        "*   Reassuring that the necessary fields are all non-null\n",
        "*   Creating a column called invoice which represents the ID for an invoice which includes all the purchases of a particular person in a single day.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTq2SS69XzEQ",
        "outputId": "513f57a1-4028-4365-f542-f8c040228197"
      },
      "source": [
        "data = dataset[[\"Doc. Date\", \"Material\",\"Ship-to nu\", \"Groupement\", \"Order qty\"]]\n",
        "\n",
        "data.dropna(axis=0, subset=[\"Groupement\"], inplace=True)\n",
        "data.dropna(axis=0, subset=[\"Material\"], inplace=True)\n",
        "data.dropna(axis=0, subset=[\"Ship-to nu\"], inplace=True)\n",
        "\n",
        "data[\"invoice\"] = data[\"Doc. Date\"].astype(str)+'m'+data[\"Ship-to nu\"].astype(str)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkq05obpDkgN",
        "outputId": "b12c995e-033b-4be6-f40e-1cf932017aeb"
      },
      "source": [
        "print(buyers)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[29804035, 29379593, 29379089, 29379604, 29379608, 29379609, 29379610, 29379611, 29379612, 29379613, 29379101, 29379615, 29607967, 29379618, 29379619, 29379620, 29379621, 29379622, 29379623, 29379625, 29379626, 29379627, 29379116, 29379628, 29379632, 29379633, 29379635, 29379639, 29379641, 29379642, 29986364, 29724733, 29379646, 29750335, 29379650, 29379651, 29839429, 29379657, 29379658, 29379661, 29379663, 29379664, 29801553, 29379666, 29379667, 29379668, 29379671, 29379672, 29379673, 29379676, 29379677, 40016990, 29379678, 29379680, 29379681, 29379682, 29379684, 29564005, 29840997, 29379687, 29563495, 29379686, 29379690, 29379692, 29379694, 29379695, 29379696, 29430384, 29379698, 29446255, 29379701, 29379702, 29379708, 29610633, 29799562, 29983887, 29643421, 29780131, 29379236, 29563563, 29793472, 40007881, 29379283, 29988565, 29727962, 29378784, 29378785, 40002273, 29379815, 29379818, 29379819, 29379820, 29635311, 29379824, 29379826, 29379827, 29379828, 29379829, 29379830, 29379831, 29379833, 29683450, 29379835, 29379836, 29379834, 29379838, 29379840, 29379841, 29379842, 29379843, 29379844, 29379845, 29379846, 29379849, 29379850, 29379853, 29379855, 29379857, 29379858, 29379859, 29379860, 29379861, 29379862, 29379863, 29379865, 29379866, 29630747, 29832988, 29379360, 29379363, 29596966, 29379377, 29379894, 29379896, 29379901, 29379905, 40001857, 29722953, 29379915, 29379916, 29378892, 29378895, 29378896, 29431121, 29756769, 29379437, 29378933, 29595514, 29801344, 29974413, 29378966, 29378967, 29563809, 29563814, 29563817, 29563820, 29378992, 29563827, 29563830, 29563832, 29563833, 29563834, 29379001, 29563836, 29563837, 29379002, 29996995, 29996996, 29379015, 29606863, 29379537, 29379538, 29537235, 29425623, 29379556, 29833190, 29379566, 29379058, 29379067]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T776mt4VxKH"
      },
      "source": [
        "A function that hot encodes all values to 0 and 1 defined to deduce if a material is being included in that particular buy (order quantity>0) or not. (no such order or order quatity=0) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHYaWPucR5_h"
      },
      "source": [
        " def hot_encode(x):\n",
        "        if(x <= 0):\n",
        "            return 0\n",
        "        if(x >= 1):\n",
        "            return 1"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNBFzwSTWbUj"
      },
      "source": [
        "Declaring a list of dictionaries. Each dictionary represents a particular groupement containing lists of recommendations for a particular material ID. It was observed that the patterns gained more support after the groupement filter. It is obvious that customers belonging to different groupements have different buying patterns. Without the filter the patterns had reduced amount of confidence and support."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OffmkaGuWrpA"
      },
      "source": [
        "dictlist = [dict() for x in range(5)]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCebsInaXi4S"
      },
      "source": [
        "Apriori algorithm steps:\n",
        "\n",
        "\n",
        "*   Creating a 2-D matrix called basket with the columns representing unique invoices and rows representing unique material IDs. For each material ID, the value of a cell is one if it was present during the partcular order, else zero.\n",
        "*   Apriori function takes up all frequently found items and filters it with min_support value of 0.02.\n",
        "*   Further association rules are established with a minimum confidence of 0.1 just to ensure all patterns are observed.\n",
        "*   Of all the patterns the patterns including a single antecedant and a single consequent are filtered since our recommendations would be one-one. Rest patterns can be used for **bundling** and can easily extracted using consequent and antecedent length.\n",
        "* Further the sorting is done so that patterns/rules with high confidence,lift and support metrices are obtained.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3MKBYuLWbuK"
      },
      "source": [
        "def encode_units(x):\n",
        "    if x <= 0:\n",
        "        return 0\n",
        "    if x >= 1:\n",
        "        return 1\n",
        "for i in range(5):\n",
        "   \n",
        "    basket = (data[data['Groupement'] == groups[i]]\n",
        "                .groupby([\"invoice\", 'Material'])['Order qty']\n",
        "                .sum().unstack().reset_index().fillna(0)\n",
        "                .set_index('invoice'))\n",
        "    basket_encoded = basket.applymap(hot_encode)\n",
        "    frq_items = apriori(basket_encoded, min_support=0.04, use_colnames=True)\n",
        "    rules = association_rules(frq_items, metric=\"confidence\", min_threshold=0.1)\n",
        "    rules = rules.sort_values(['confidence', 'lift'], ascending=[False, False])\n",
        "    rules[\"antecedent_len\"] = rules[\"antecedents\"].apply(lambda x: len(x))\n",
        "    rules[\"consequent_len\"] = rules[\"consequents\"].apply(lambda x: len(x))\n",
        "    rule1 = rules[(rules['antecedent_len']+rules['consequent_len'] == 2)]\n",
        "    rule1 = rule1.sort_values(['confidence', 'lift', 'support'], ascending=[False, False, False])\n",
        "    rule = rule1.iloc[:, :].values\n",
        "    from collections import defaultdict\n",
        "    dictt = defaultdict(list)\n",
        "    for z in rule:\n",
        "      for j in z[0]:\n",
        "          for k in z[1]:\n",
        "              dictt[j].append([z[5], k])\n",
        "    dictlist[i] = dictt\n",
        "  \n",
        "    \n",
        "                                                                           "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUConCLJFIjz",
        "outputId": "866c3354-76fa-455b-a225-217b2356b880"
      },
      "source": [
        "dictlist[0][3338]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.5846153846153846, 59848],\n",
              " [0.5538461538461539, 11574],\n",
              " [0.49230769230769234, 9974],\n",
              " [0.47692307692307695, 71317],\n",
              " [0.4615384615384615, 3337],\n",
              " [0.4615384615384615, 3410],\n",
              " [0.4615384615384615, 6013],\n",
              " [0.4461538461538462, 10946],\n",
              " [0.4307692307692308, 10957],\n",
              " [0.4153846153846154, 3372]]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IUoQb5fbVqf"
      },
      "source": [
        "It is observed that not all material IDs are being part of market patterns. Such cases can be tackled by recommending materials which are similar. This can be deduced using cosine similarity between the materials\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKHiRzytb3hl"
      },
      "source": [
        "***One where the Hectolitres get predicted through auto-encoder***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iCX8PcOKI-C"
      },
      "source": [
        "Preprocessing: \n",
        "1. Finding average Hectolitres of each material ordered by each Wholesaler \n",
        "2. Scaling them using minmax technique column-wise\n",
        "3. Pytorch Models accept Float tensors only.So converting the numpy array into a tesor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nHyGo50b4EH"
      },
      "source": [
        "data =dataset.iloc[:, :].values\n",
        "x = np.zeros((179, 163))\n",
        "y = np.zeros((179, 163))\n",
        "for boom in data:\n",
        "        if(boom[3] > 0):\n",
        "            x[buyers.index(boom[5])][materials.index(boom[3])] += boom[7]\n",
        "            y[buyers.index(boom[5])][materials.index(boom[3])] += 1\n",
        "for i in range(179):\n",
        "    for j in range(163):\n",
        "        if y[i][j] > 0:\n",
        "            x[i][j] /= y[i][j]\n",
        "maxes = np.amax(x, axis=1)\n",
        "mines = np.amin(x, axis=1)\n",
        "for i in range(179):\n",
        "    for j in range(163):\n",
        "        x[i][j] -= mines[i]\n",
        "        x[i][j] /= (maxes[i]-mines[i]) \n",
        "xsc = torch.FloatTensor(x)        "
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOr36uJ4QIC-"
      },
      "source": [
        "Model:\n",
        "* The autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input.\n",
        "* Performing the copying task perfectly would simply duplicate the signal, and this is why autoencoders usually are restricted in ways that force them to reconstruct the input approximately(here number of hidden nodes are less than the input size preventing direct duplication) ,preserving only the most relevant aspects of the data in the copy.\n",
        "* Here it tries to copy the patterns in delivered amounts considering the average HLDelivered of a material purchased by a wholsaler in the past. \n",
        "1. Creating a class to build a stacked auto-encoder.\n",
        "2. Initializing three an input layer, two hidden layers and an output layer with the mentioned number of nodes.\n",
        "3. Declaring sigmoid as the activation function.\n",
        "4. The forward function involves passing the tensor x through the layers activated by sigmoid function. (Note that fc4 is the output layer and thus will work with linear activation). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVA_YOodQIQq"
      },
      "source": [
        "class SAE(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(SAE, self).__init__()\n",
        "            self.fc1 = nn.Linear(163, 25)\n",
        "            self.fc2 = nn.Linear(25, 12)\n",
        "            self.fc3 = nn.Linear(12, 30)\n",
        "            self.fc4 = nn.Linear(30, 163)\n",
        "            self.activation = nn.Sigmoid()\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.activation(self.fc1(x))\n",
        "            x = self.activation(self.fc2(x))\n",
        "            x = self.activation(self.fc3(x))\n",
        "            x = self.fc4(x)\n",
        "            return x"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW3jV6UHR5oW"
      },
      "source": [
        "Declaring training parameters: \n",
        "* Mean squared error as the metrics for training.\n",
        "* Learning rate of 0.06 and weight decay of 0.5\n",
        "*   Number of epochs in training = 50\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sMZhyvoQYAY"
      },
      "source": [
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimiser = optim.RMSprop(sae.parameters(), lr=0.06, weight_decay=0.5)\n",
        "nb_epochs = 50"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY0ixX4DSUh-"
      },
      "source": [
        "Training Step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYYEowV7SUwO",
        "outputId": "8d1203c2-7708-41fb-f1df-1f684cd4750e"
      },
      "source": [
        "for epochs in range(1,nb_epochs+1):\n",
        "    training_loss=0\n",
        "    s=0.\n",
        "    for users in range(178):\n",
        "        xx=Variable(xsc[users]).unsqueeze(0)\n",
        "        target=xx.clone()\n",
        "        if torch.sum(target.data > 0) > 0:\n",
        "            output=sae(xx)\n",
        "            target.requires_grad=False\n",
        "            output[target ==0] = 0\n",
        "            loss=criterion(output,target)\n",
        "            mean_corrector=(179)/ float(torch.sum(target.data>0)+1e-10)\n",
        "            loss.backward()\n",
        "            training_loss+=np.sqrt(loss.data * mean_corrector)\n",
        "            s+=1.  \n",
        "            optimiser.step()\n",
        "    print(\"Epoch:\"+str(epochs)+\" Loss: \"+str(training_loss/s))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1 Loss: tensor(0.2746)\n",
            "Epoch:2 Loss: tensor(0.2616)\n",
            "Epoch:3 Loss: tensor(0.2079)\n",
            "Epoch:4 Loss: tensor(0.1938)\n",
            "Epoch:5 Loss: tensor(0.1878)\n",
            "Epoch:6 Loss: tensor(0.1842)\n",
            "Epoch:7 Loss: tensor(0.1820)\n",
            "Epoch:8 Loss: tensor(0.1803)\n",
            "Epoch:9 Loss: tensor(0.1792)\n",
            "Epoch:10 Loss: tensor(0.1781)\n",
            "Epoch:11 Loss: tensor(0.1776)\n",
            "Epoch:12 Loss: tensor(0.1770)\n",
            "Epoch:13 Loss: tensor(0.1764)\n",
            "Epoch:14 Loss: tensor(0.1758)\n",
            "Epoch:15 Loss: tensor(0.1757)\n",
            "Epoch:16 Loss: tensor(0.1752)\n",
            "Epoch:17 Loss: tensor(0.1750)\n",
            "Epoch:18 Loss: tensor(0.1745)\n",
            "Epoch:19 Loss: tensor(0.1746)\n",
            "Epoch:20 Loss: tensor(0.1740)\n",
            "Epoch:21 Loss: tensor(0.1742)\n",
            "Epoch:22 Loss: tensor(0.1736)\n",
            "Epoch:23 Loss: tensor(0.1738)\n",
            "Epoch:24 Loss: tensor(0.1734)\n",
            "Epoch:25 Loss: tensor(0.1735)\n",
            "Epoch:26 Loss: tensor(0.1732)\n",
            "Epoch:27 Loss: tensor(0.1733)\n",
            "Epoch:28 Loss: tensor(0.1730)\n",
            "Epoch:29 Loss: tensor(0.1731)\n",
            "Epoch:30 Loss: tensor(0.1728)\n",
            "Epoch:31 Loss: tensor(0.1729)\n",
            "Epoch:32 Loss: tensor(0.1726)\n",
            "Epoch:33 Loss: tensor(0.1728)\n",
            "Epoch:34 Loss: tensor(0.1724)\n",
            "Epoch:35 Loss: tensor(0.1727)\n",
            "Epoch:36 Loss: tensor(0.1724)\n",
            "Epoch:37 Loss: tensor(0.1727)\n",
            "Epoch:38 Loss: tensor(0.1721)\n",
            "Epoch:39 Loss: tensor(0.1724)\n",
            "Epoch:40 Loss: tensor(0.1721)\n",
            "Epoch:41 Loss: tensor(0.1723)\n",
            "Epoch:42 Loss: tensor(0.1721)\n",
            "Epoch:43 Loss: tensor(0.1718)\n",
            "Epoch:44 Loss: tensor(0.1720)\n",
            "Epoch:45 Loss: tensor(0.1722)\n",
            "Epoch:46 Loss: tensor(0.1719)\n",
            "Epoch:47 Loss: tensor(0.1721)\n",
            "Epoch:48 Loss: tensor(0.1719)\n",
            "Epoch:49 Loss: tensor(0.1720)\n",
            "Epoch:50 Loss: tensor(0.1718)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqOAmLKJSrs-"
      },
      "source": [
        "Output: Converting the output tensor to list and reversing the scaling operation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVud-wiPSs-p"
      },
      "source": [
        "output = sae(xsc)\n",
        "ans = output.tolist()\n",
        "for i in range(179):\n",
        "    for j in range(163):\n",
        "        ans[i][j] *= (maxes[i]-mines[i])\n",
        "        ans[i][j] += mines[i]\n",
        "        if(ans[i][j]<0):\n",
        "            ans[i][j]=0.00001"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "netEmcBoSVWx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ymU1JVATJYs"
      },
      "source": [
        "***One with the Restricted Boltzmann Machine***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CisK6abKmRm"
      },
      "source": [
        "Preprocessing:\n",
        "\n",
        "\n",
        "1.   Creating a numpy array of total materials*total wholesalers.Each entry is one if the corresponding material has been bought by the wholesaler in the past.(If we had data about the rating or review recieved from the wholesaler, the array would then depict the liking of materials.)\n",
        "2. Converting the array into float tensor which is the desired input to the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC_p3ZOqTQBx"
      },
      "source": [
        "x = np.zeros((179, 179))\n",
        "for boom in data:\n",
        "      if(boom[4] > 0 and boom[7] > 0):\n",
        "            x[buyers.index(boom[5])][materials.index(boom[3])] = 1\n",
        "xsc = torch.FloatTensor(x)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEp9NOV4yqRB"
      },
      "source": [
        "Model:\n",
        "* A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.\n",
        "* The standard type of RBM has binary-valued (Boolean) hidden and visible units, and consists of a matrix of weights W of size m*n. \n",
        "* Each weight element (w[i][j])  of the matrix is associated with the connection between the visible (input) unit v[i] and the hidden unit h[j]. In addition, there are bias weights (offsets) a[i] for v[i]  and b[j] for h[j]. \n",
        "\n",
        "\n",
        "1.   init function initializes weights and biases randomly \n",
        "2.   sample_h produces probabilities (ph_given_v) of hidden nodes h[j] given the state of visible nodes. It also outputs bernoulli samples corresponding to the calculated probabilities.\n",
        "3.  sample_v performs the same functions as sample_h just with hidden nodes as input and visible nodes in the output end.\n",
        "4. Train function uses the principle of contrastive divergence and aligns the weights and biases to minimize the energy of the state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_4nPrzPywXA"
      },
      "source": [
        "class RBM():\n",
        "                    def __init__(self, nv, nh):\n",
        "                        self.W = torch.randn(nh, nv)\n",
        "                        self.A = torch.randn(1, nh)\n",
        "                        self.B = torch.randn(1, nv)\n",
        "\n",
        "                    def sample_h(self, x):\n",
        "                        wx = torch.mm(x, self.W.t())\n",
        "                        activation = wx + self.A.expand_as(wx)\n",
        "                        p_h_given_v = torch.sigmoid(activation)\n",
        "                        return p_h_given_v, torch.bernoulli(p_h_given_v)\n",
        "\n",
        "                    def sample_v(self, y):\n",
        "                        wy = torch.mm(y, self.W)\n",
        "                        activation = wy + self.B.expand_as(wy)\n",
        "                        p_v_given_h = torch.sigmoid(activation)\n",
        "                        return p_v_given_h, torch.bernoulli(p_v_given_h)\n",
        "\n",
        "                    def train(self, v0, vk, ph0, phk):\n",
        "                        self.W += (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t()\n",
        "                        self.B += torch.sum((v0-vk), 0)\n",
        "                        self.A += torch.sum((ph0-phk), 0)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5Wz_w471UMQ"
      },
      "source": [
        "Parameters for training:\n",
        "\n",
        "\n",
        "*   Number of columns and rows are both less than 200 and hence a bath size of 1 is sufficient and as expected gave better results.\n",
        "*   As per previous RBM users considering number of hidden nodes to be nearly equal to half of the number of visible nodes gives higher accuracy. So nh is chosen to be 90 and also obtained better results.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-Vx98se1UZo"
      },
      "source": [
        "batch_size = 1\n",
        "rbm = RBM(179, 90)\n",
        "nb_epochs = 15"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBzY40q22NOG"
      },
      "source": [
        "Training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFP_CVjF2NeH",
        "outputId": "94ce77f5-9737-47f9-aaae-902834ab3ee3"
      },
      "source": [
        "for i in range(1, nb_epochs+1):\n",
        "        train_loss = 0\n",
        "        s = 0.\n",
        "        for id_user in range(1, 179-batch_size, batch_size):\n",
        "            vk = xsc[id_user:id_user+batch_size]\n",
        "            v0 = xsc[id_user:id_user+batch_size]\n",
        "            ph0, _ = rbm.sample_h(v0)\n",
        "            for k in range(10):\n",
        "                _, hk = rbm.sample_h(vk)\n",
        "                _, vk = rbm.sample_v(hk)\n",
        "                vk[v0 < 0] = v0[v0 < 0]\n",
        "            phk, _ = rbm.sample_h(vk)\n",
        "            rbm.train(v0, vk, ph0, phk)\n",
        "            train_loss += torch.mean(torch.abs(v0[v0 >= 0]-vk[v0 >= 0]))\n",
        "            s += 1.\n",
        "        print(\"epoch: \"+str(i)+\" loss: \"+str(train_loss/s))   "
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 loss: tensor(0.2134)\n",
            "epoch: 2 loss: tensor(0.2011)\n",
            "epoch: 3 loss: tensor(0.1957)\n",
            "epoch: 4 loss: tensor(0.1919)\n",
            "epoch: 5 loss: tensor(0.1882)\n",
            "epoch: 6 loss: tensor(0.1857)\n",
            "epoch: 7 loss: tensor(0.1800)\n",
            "epoch: 8 loss: tensor(0.1828)\n",
            "epoch: 9 loss: tensor(0.1799)\n",
            "epoch: 10 loss: tensor(0.1773)\n",
            "epoch: 11 loss: tensor(0.1769)\n",
            "epoch: 12 loss: tensor(0.1736)\n",
            "epoch: 13 loss: tensor(0.1793)\n",
            "epoch: 14 loss: tensor(0.1782)\n",
            "epoch: 15 loss: tensor(0.1773)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41fgSt8a2r7i"
      },
      "source": [
        "Output and Metrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QZfAgs_2rEe",
        "outputId": "2c45b030-7e30-436f-dd5d-8bbcf8845c7e"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "ans=[]\n",
        "avg_rek=0\n",
        "avg_prk=0\n",
        "conmatr=[[0,0],[0,0]]\n",
        "for id_user in range(179):\n",
        "    v=xsc[id_user:id_user+1]\n",
        "    _,h=rbm.sample_h(v)\n",
        "    pv,v1=rbm.sample_v(h)\n",
        "    pvl=pv.tolist()\n",
        "    pvn=np.array(pvl)\n",
        "    indones=pvn[0].argsort()[-25:][::-1]\n",
        "    rel=0\n",
        "    for iii in indones:\n",
        "        if x[id_user][iii]>0 and v1[0][iii]==1:\n",
        "            rel=rel+1\n",
        "    conmatr+=confusion_matrix(v[0].tolist(),v1[0].tolist())   \n",
        "    avg_rek+=rel/float(torch.sum(v))\n",
        "    avg_prk+=rel/25\n",
        "    \n",
        "    ans.append(pvl)\n",
        "conmatr\n",
        "avg_rek/=179\n",
        "avg_prk/=179\n",
        "print(conmatr,avg_rek,avg_prk)    "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[21717  2214]\n",
            " [ 3062  5048]] 0.4781714596353773 0.7405586592178772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7_xyznTATdn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}